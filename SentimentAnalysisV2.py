# Version 2 of Sentiment Analysis approach
# Planned process:
    # Split dataset into 80/20 (would 80/10/10 be better for training/test/validation?) 
    # Vader used to classify sentiment polarity + intensity  
    # TF-IDF then captures word importance 
    # Run these for every headline in 80% 
    # Together they are the vectors for training model 
    # Train logistic regression model or Naive Bayes model on these features 
    # Test model on remaining 20% test data  
    # Measure model performance using accuracy, and f1 score 

# Dataset is currently 600 headlines generated by ChatGPT
# 200 pos, 200 neg, 200 neu

# Shuffle and split dataset into 80/10/10
# May change this later to 80/20

import pandas as pd
from sklearn.model_selection import train_test_split

def shuffle_and_split_dataset(file_path, sheet_name="Dataset", train_size=0.8, val_size=0.1, test_size=0.1, random_state=42):
    """
    Loads, shuffles, and splits the dataset into training, validation, and test sets.

    Parameters:
    - file_path (str): Path to the Excel file containing the original dataset.
    - sheet_name (str): Sheet name of original dataset.
    - train_size (float): Proportion of the dataset to use for training - 80%.
    - val_size (float): Proportion of the dataset to use for validation - 10%.
    - test_size (float): Proportion of the dataset to use for testing - 10%.
    - random_state (int): Common convention to set it as 42 whic ensures reproducability.

    Saves:
    - Train_Dataset.xlsx
    - Validation_Dataset.xlsx
    - Test_Dataset.xlsx
    """
    # Load dataset
    df = pd.read_excel(file_path, sheet_name=sheet_name)
    
    # Display initial class distribution to ensure 200/200/200
    print("Initial Class Distribution:\n", df["Labelled Rating"].value_counts())

    # Shuffle dataset
    # Random state is 42, thank Hitchhikers guide to the galaxy 
    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)

    # Ensure train/val/test split sums to 1
    assert train_size + val_size + test_size == 1, "Train, validation, and test sizes must sum to 1."

    # Split dataset into training (80%) and temp (20%)
    # Stratify command ensures that Labelled Rating is equally split for each section which hopefully reduces overfitting
    train_df, temp_df = train_test_split(df, test_size=(1 - train_size), random_state=random_state, stratify=df["Labelled Rating"])
    
    # Split temp into validation (10%) and test (10%)
    val_df, test_df = train_test_split(temp_df, test_size=(test_size / (val_size + test_size)), random_state=random_state, stratify=temp_df["Labelled Rating"])

    # Save splits into separate Excel files
    train_df.to_excel("Train_Dataset.xlsx", index=False)
    val_df.to_excel("Validation_Dataset.xlsx", index=False)
    test_df.to_excel("Test_Dataset.xlsx", index=False)

    # Display split sizes
    print(f"Training set: {len(train_df)} headlines")
    print(f"Validation set: {len(val_df)} headlines")
    print(f"Test set: {len(test_df)} headlines")

# MAIN FUNCTIONS TO RUN
#shuffle_and_split_dataset("Dataset.xlsx")
