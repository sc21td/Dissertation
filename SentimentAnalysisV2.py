# Version 2 of Sentiment Analysis approach
# Planned process:
    # Split dataset into 80/20 (would 80/10/10 be better for training/test/validation?) 
    # Vader used to classify sentiment polarity + intensity  
    # TF-IDF then captures word importance 
    # Run these for every headline in 80% 
    # Together they are the vectors for training model 
    # Train logistic regression model or Naive Bayes model on these features 
    # Test model on remaining 20% test data  
    # Measure model performance using accuracy, and f1 score 

# Dataset is currently 600 headlines generated by ChatGPT
# 200 pos, 200 neg, 200 neu

# Used for reading in the data
import pandas as pd
# Scikit learn used for ML
from sklearn.model_selection import train_test_split
# VADER classification
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# TD-IDF Vectorising
from sklearn.feature_extraction.text import TfidfVectorizer
# Logistic Regression
from sklearn.linear_model import LogisticRegression
# Label encoder used for encoding pos,neg, neut in training
from sklearn.preprocessing import LabelEncoder
# For model evaluation
from sklearn.metrics import accuracy_score, f1_score, classification_report

def shuffle_and_split_dataset(file_path, sheet_name="Dataset", train_size=0.8, val_size=0.1, test_size=0.2, random_state=42):
    """
    Loads, shuffles, and splits the dataset into training, validation, and test sets.

    Parameters:
    - file_path (str): Path to the Excel file containing the original dataset.
    - sheet_name (str): Sheet name of original dataset.
    - train_size (float): Proportion of the dataset to use for training - 80%.
    - val_size (float): Proportion of the dataset to use for validation - 10%.
    - test_size (float): Proportion of the dataset to use for testing - 10%.
    - random_state (int): Common convention to set it as 42 whic ensures reproducability.

    Saves:
    - Train_Dataset.xlsx
    - Validation_Dataset.xlsx
    - Test_Dataset.xlsx
    """
    # Load dataset
    df = pd.read_excel(file_path, sheet_name=sheet_name)
    
    # Display initial class distribution to ensure 200/200/200
    print("Initial Class Distribution:\n", df["Labelled Rating"].value_counts())

    # Shuffle dataset
    # Random state is 42, thank Hitchhikers guide to the galaxy 
    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)

    # Ensure train/val/test split sums to 1
    #assert train_size + val_size + test_size == 1, "Train, validation, and test sizes must sum to 1."

    # Ensure train/test split sums to 1
    assert train_size + test_size == 1, "Train and test sizes must sum to 1."

    # Split dataset into training (80%) and temp (20%)
    # Stratify command ensures that Labelled Rating is equally split for each section which hopefully reduces overfitting
    #train_df, temp_df = train_test_split(df, test_size=(1 - train_size), random_state=random_state, stratify=df["Labelled Rating"])
    
    # Split temp into validation (10%) and test (10%)
    #val_df, test_df = train_test_split(temp_df, test_size=(test_size / (val_size + test_size)), random_state=random_state, stratify=temp_df["Labelled Rating"])

    # Split into 80/20 split
    train_df, test_df = train_test_split(df, test_size=(1 - train_size), random_state=random_state, stratify=df["Labelled Rating"])

    # Save splits into separate Excel files
    train_df.to_excel("Train_DatasetV2.xlsx", index=False)
    #val_df.to_excel("Validation_Dataset.xlsx", index=False)
    test_df.to_excel("Test_DatasetV2.xlsx", index=False)

    # Display split sizes
    print(f"Training set: {len(train_df)} headlines")
    #print(f"Validation set: {len(val_df)} headlines")
    print(f"Test set: {len(test_df)} headlines")

def apply_vader(df, text_column="Statement"):
    """
    Applies VADER label headlines with sentiment scores

    Parameters:
    - df (pd.DataFrame): DataFrame containing the headlines.
    - text_column (str): The name of the column containing the text data.

    Returns:
    - df (pd.DataFrame): DataFrame with added VADER sentiment score columns.
    """
    # Initialise VADER
    analyzer = SentimentIntensityAnalyzer()

    # Function to get VADER scores
    def get_vader_scores(text):
        scores = analyzer.polarity_scores(text)
        return pd.Series([scores['pos'], scores['neg'], scores['neu'], scores['compound']])

    # Apply VADER to the dataset
    df[['pos', 'neg', 'neu', 'compound']] = df[text_column].apply(get_vader_scores)
    
    return df

def apply_tfidf(df, text_column="Statement", max_features=500):
    """
    Converts text into numerical TF-IDF features.

    Parameters:
    - df (pd.DataFrame): DataFrame containing the text data.
    - text_column (str): The name of the column containing the text.
    - max_features (int): The maximum number of words to keep in the TF-IDF vectorisation.

    Returns:
    - tfidf_df (pd.DataFrame): DataFrame containing TF-IDF features.
    - vectorizer (TfidfVectorizer): Fitted TF-IDF vectorizer for future transformations.
    """
    # Initialise TF-IDF vectoriser
    vectoriser = TfidfVectorizer(max_features=max_features)

    # Fit and transform the text data
    tfidf_matrix = vectoriser.fit_transform(df[text_column])

    # Convert TF-IDF matrix to DataFrame
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectoriser.get_feature_names_out())

    return tfidf_df, vectoriser

def train_model(train_file="Train_Dataset_FinalV2.xlsx"):
    """
    Trains a sentiment classification model using either Logistic Regression or Na√Øve Bayes.

    Parameters:
    - train_file (str): Path to the processed training dataset (with VADER & TF-IDF features).

    Returns:
    - model: The trained classifier.
    - X_train (pd.DataFrame): Training feature set.
    - y_train (pd.Series): Training labels.
    - label_encoder (LabelEncoder): Encoder used to transform sentiment labels.
    """

    # Load the training dataset
    df = pd.read_excel(train_file)

    # Extract features (VADER + TF-IDF) and labels
    # X features are VADER and TF-IDF data points
    X_train = df.drop(columns=["Labelled Rating"]) 
    # Y features are the sentiment labels 
    y_train = df["Labelled Rating"]

    # Encode sentiment labels as numerical values
    label_encoder = LabelEncoder()
    # Converts labels (positive, negative, neutral) into numbers
    y_train = label_encoder.fit_transform(y_train)  

    # Logistic regression model, currently set to 1000 iterations
    model = LogisticRegression(max_iter=1000)

    # Train the model
    model.fit(X_train, y_train) 

    print("Model training completed")
    
    return model, X_train, y_train, label_encoder

def test_model(model, vectoriser, label_encoder, test_file="Test_DatasetV2.xlsx"):
    """
    Tests the trained model on unseen test data.

    Parameters:
    - model: Trained classification model.
    - vectoriser: TF-IDF vectoriser used during training.
    - label_encoder: Encoder used to transform sentiment labels.
    - test_file (str): Path to the test dataset.

    Returns:
    - y_true (list): Actual sentiment labels.
    - y_pred (list): Predicted sentiment labels.
    """
    # Load test dataset
    test_df = pd.read_excel(test_file)

    # Extract VADER sentiment scores
    test_df = apply_vader(test_df)

    # Extract TF-IDF features using the same vectoriser from training
    tfidf_matrix = vectoriser.transform(test_df["Statement"])
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectoriser.get_feature_names_out())

    # Combine VADER and TF-IDF features
    X_test = pd.concat([test_df[['pos', 'neg', 'neu', 'compound']], tfidf_df], axis=1)

    # Convert labels to numerical values
    y_true = label_encoder.transform(test_df["Labelled Rating"])

    # Predict sentiment
    y_pred = model.predict(X_test)

    return y_true, y_pred

def evaluate_model(y_true, y_pred, label_encoder):
    """
    Evaluates the model's performance using accuracy and F1 score.

    Parameters:
    - y_true (list): Actual sentiment labels.
    - y_pred (list): Predicted sentiment labels.
    - label_encoder (LabelEncoder): Encoder used to transform sentiment labels.

    Returns:
    - Prints out evaluation metrics.
    """
    # Calculate accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Model Accuracy: {accuracy:.2f}")

    # Calculate F1 Score
    f1 = f1_score(y_true, y_pred, average="weighted")
    print(f"F1 Score: {f1:.2f}")

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))


# MAIN FUNCTIONS TO RUN

if __name__ == "__main__":

    # 1) Shuffle the whole dataset
    shuffle_and_split_dataset("Dataset.xlsx")
    print("Shuffled dataset")

    # 2) Load training dataset
    train_df = pd.read_excel("Train_DatasetV2.xlsx")
    print("Dataset loaded")

    # # 3) Apply VADER
    train_df = apply_vader(train_df)
    print("VADER applied successfully")

    # # 4) Apply TF-IDF
    tfidf_df, vectoriser = apply_tfidf(train_df)
    print("TF-IDF applied successfully")

    # # 5) Combine VADER and TF-IDF features 
    final_train_df = pd.concat([train_df[['Labelled Rating','pos', 'neg', 'neu', 'compound']], tfidf_df], axis=1)
    print("Features combined for final training excel")

    # # 6) Save processed dataset into Excel
    final_train_df.to_excel("Train_Dataset_FinalV2.xlsx", index=False)
    print("VADER and TF-IDF processing completed and saved successfully")

    # 7) Train model using preprocessed dataset
    model, X_train, y_train, label_encoder = train_model()
    print ("Step 7 complete")
    
    # 8) Test model on 10% test set (60 heasdlines)
    y_true, y_pred = test_model(model, vectoriser, label_encoder)
    print("Testing set done and dusted")

    # 9) Evaluate the model on Accuracy and F1 score
    evaluate_model(y_true, y_pred, label_encoder)
